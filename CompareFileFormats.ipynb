{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pyar\n",
    "import pyarrow.parquet as pyarpq\n",
    "import pyarrow.dataset as pyards\n",
    "import pyarrow.csv as pyarcsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "Using the Parquet Best Practices. We will be using the SalesOrderItems whic has 1930 rows with the below schema.\n",
    "SALESORDERID: SALESORDERITEM: PRODUCTID: NOTEID: CURRENCY: GROSSAMOUNT:,NETAMOUNT: ,TAXAMOUNT: ,ITEMATPSTATUS: ,OPITEMPOS: ,QUANTITY: ,QUANTITYUNIT: ,DELIVERYDATE: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the original DataFrame:1930\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "csv_file_path = '/Users/krisnunes/Study/python/DataEngineering/DataLakeHouse/FileFormat/Avro/Play/SalesOrderItems.csv'  # Update with your CSV file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "print(f\"Check the original DataFrame:{len(df)}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large CSV File Generation\n",
    "I used combinations of looping throgh files to generate a file with 15,440,000 (fifteen million, four hundred forty thousand) rows.\n",
    "Size of file: 6.52GB\n",
    "Type: CSV\n",
    "Obviously this took a long time to generate.\n",
    "After compression: 170.MB\n",
    "Compression ratio: ~40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "csv_file_path = '/Users/krisnunes/Study/python/DataEngineering/DataLakeHouse/FileFormat/Avro/Play/SalesOrdersItems15440000.csv'  # Update with your CSV file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "print(f\"Check the original DataFrame:{len(df)}\");\n",
    "# Duplicate the DataFrame\n",
    "df_duplicate = df.copy()\n",
    "\n",
    "# Optionally, modify the duplicated DataFrame to differentiate the records\n",
    "# Example: Add a suffix to the 'NOTEID' field in the duplicated DataFrame\n",
    "for i in range(1,4):\n",
    "    print(i)\n",
    "    df_duplicate['NOTEID'] = df_duplicate['NOTEID'] + f'_dup{i}'\n",
    "    # Combine the original and duplicated DataFrames\n",
    "    df_combined = pd.concat([df, df_duplicate], ignore_index=True)\n",
    "    df_duplicate = df_combined;\n",
    "\n",
    "# Check the combined DataFrame\n",
    "print(f\"Check the combined DataFrame:{len(df_combined)}\");\n",
    "\n",
    "\n",
    "# Optionally, save the combined DataFrame to a new CSV file\n",
    "combined_csv_file_path = '/Users/krisnunes/Study/python/DataEngineering/DataLakeHouse/FileFormat/Avro/Play/DataFrame:3860000.csv'  # Update with your desired output CSV file path\n",
    "df_combined.to_csv(combined_csv_file_path, index=False)\n",
    "\n",
    "print(f'Combined data saved to {combined_csv_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salesOrder = pd.read_csv('SalesOrdersItems15440000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet\n",
    "I built the schema based on a previous demo and reusing it.\n",
    "SALESORDERID: int64,SALESORDERITEM: int64,PRODUCTID: string,NOTEID: string,CURRENCY: string,GROSSAMOUNT: int64,NETAMOUNT: double,TAXAMOUNT: double,ITEMATPSTATUS: string,OPITEMPOS: string,QUANTITY: int64,QUANTITYUNIT: string,DELIVERYDATE: int64\n",
    "Size of file: 11.1 MB\n",
    "Type: Parquet\n",
    "Time: <1 min\n",
    "Compression Ratio: 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reading the schema from the file\n",
    "myschema = pyarpq.read_schema(\"SalesOrderItems.metadata\")\n",
    "\n",
    "# Convert the DataFrame to a PyArrow Table using the schema\n",
    "sales_order_table = pyar.Table.from_pandas(df_salesOrder, schema=myschema)\n",
    "pyarpq.write_table(\n",
    "    sales_order_table,\n",
    "    'SalesOrderItems_base.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to read\n",
    "Time: 40s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15440000\n"
     ]
    }
   ],
   "source": [
    "df_parquetread = pd.read_parquet('SalesOrderItems_base.parquet') \n",
    "print(len(df_parquetread))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avro: fastavro\n",
    "Used a similar schema from Parquet. Used fastavro \n",
    "Size of file: 6.3 GB\n",
    "Type: avro\n",
    "Time: 6 mins\n",
    "Compression Ratio: 1.0\n",
    "After compression:\n",
    "After compression: 178.MB\n",
    "Compression ratio: ~40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data has been successfully converted to output.avro\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fastavro\n",
    "import numpy as np\n",
    "\n",
    "# Define the Avro schema\n",
    "schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"SalesOrder\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"SALESORDERID\", \"type\": \"long\"},\n",
    "        {\"name\": \"SALESORDERITEM\", \"type\": \"long\"},\n",
    "        {\"name\": \"PRODUCTID\", \"type\": \"string\"},\n",
    "        {\"name\": \"NOTEID\", \"type\": \"string\"},\n",
    "        {\"name\": \"CURRENCY\", \"type\": \"string\"},\n",
    "        {\"name\": \"GROSSAMOUNT\", \"type\": \"long\"},\n",
    "        {\"name\": \"NETAMOUNT\", \"type\": \"double\"},\n",
    "        {\"name\": \"TAXAMOUNT\", \"type\": \"double\"},\n",
    "        {\"name\": \"ITEMATPSTATUS\", \"type\": \"string\"},\n",
    "        {\"name\": \"OPITEMPOS\", \"type\": \"string\"},\n",
    "        {\"name\": \"QUANTITY\", \"type\": \"long\"},\n",
    "        {\"name\": \"QUANTITYUNIT\", \"type\": \"string\"},\n",
    "        {\"name\": \"DELIVERYDATE\", \"type\": \"long\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Read the CSV file\n",
    "\n",
    "# Replace nan with an appropriate default value\n",
    "df = df_salesOrder.replace({np.nan: \"\"})\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "records = df.to_dict(orient='records')\n",
    "\n",
    "# Write the Avro file\n",
    "avro_file = 'output.avro'\n",
    "with open(avro_file, 'wb') as out:\n",
    "    fastavro.writer(out, schema, records)\n",
    "\n",
    "print(f\"CSV data has been successfully converted to {avro_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avro: Time to read\n",
    "Time: 40s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Avro file\n",
    "avro_file = 'output.avro'\n",
    "\n",
    "# Read the Avro file into a list of records\n",
    "records = []\n",
    "\n",
    "with open(avro_file, 'rb') as file:\n",
    "    reader = fastavro.reader(file)\n",
    "    for record in reader:\n",
    "        records.append(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avro: avro\n",
    "Used a similar schema from Parquet. Used avro \n",
    "Size of file: 6.3 GB\n",
    "Type: avro\n",
    "Time: 11 mins\n",
    "Compression Ratio: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15440000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import avro.schema\n",
    "import avro.datafile\n",
    "import avro.io\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the Avro schema\n",
    "schema_str = \"\"\"\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"SalesOrderItem\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"SALESORDERID\", \"type\": \"long\"},\n",
    "    {\"name\": \"SALESORDERITEM\", \"type\": \"long\"},\n",
    "    {\"name\": \"PRODUCTID\", \"type\": \"string\"},\n",
    "    {\"name\": \"NOTEID\", \"type\": \"string\"},\n",
    "    {\"name\": \"CURRENCY\", \"type\": \"string\"},\n",
    "    {\"name\": \"GROSSAMOUNT\", \"type\": \"long\"},\n",
    "    {\"name\": \"NETAMOUNT\", \"type\": \"double\"},\n",
    "    {\"name\": \"TAXAMOUNT\", \"type\": \"double\"},\n",
    "    {\"name\": \"ITEMATPSTATUS\", \"type\": \"string\"},\n",
    "    {\"name\": \"OPITEMPOS\", \"type\": \"string\"},\n",
    "    {\"name\": \"QUANTITY\", \"type\": \"long\"},\n",
    "    {\"name\": \"QUANTITYUNIT\", \"type\": \"string\"},\n",
    "    {\"name\": \"DELIVERYDATE\", \"type\": \"long\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "# Replace nan with an appropriate default value\n",
    "df_salesOrdersamll = df_salesOrder.replace({np.nan: \"\"})\n",
    "\n",
    "print(len(df_salesOrdersamll))\n",
    "\n",
    "schema = avro.schema.Parse(schema_str)\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "records = df_salesOrdersamll.to_dict(orient='records')\n",
    "\n",
    "# Write the Avro file\n",
    "avro_file = 'output.avro'\n",
    "with open(avro_file, 'wb') as out:\n",
    "    writer = avro.datafile.DataFileWriter(out, avro.io.DatumWriter(), schema)\n",
    "    for record in records:\n",
    "        writer.append(record)\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('sklearn-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56fab9006b55109f6b272e1c30f686d0786ed0b8d0311d3128cb757a4af40d16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
